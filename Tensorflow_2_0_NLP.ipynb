{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow 2.0 NLP",
      "provenance": [],
      "authorship_tag": "ABX9TyPcsQZgkNsoEZ8CNly0m+NB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prince001996/Tensorflow-2.0-learning-/blob/master/Tensorflow_2_0_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CwJgoqe0YDL",
        "colab_type": "code",
        "outputId": "0e52d4ee-7b04-4a36-c5e9-4d5ba8500391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irz1RhLAmSL7",
        "colab_type": "code",
        "outputId": "274a448c-6074-447f-e699-9f2692d360f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        " vocab = {}  \n",
        "word_encoding = 1\n",
        "def one_hot_encoding(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \") \n",
        "  encoding = []  \n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      code = vocab[word]  \n",
        "      encoding.append(code) \n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding.append(word_encoding)\n",
        "      word_encoding += 1\n",
        "  \n",
        "  return encoding\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "encoding = one_hot_encoding(text)\n",
        "print(encoding)\n",
        "print(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NuDmcEypYjU",
        "colab_type": "code",
        "outputId": "bf63cb1c-7de5-45ff-8ac6-b1d210062b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_encode = one_hot_encoding(positive_review)\n",
        "neg_encode = one_hot_encoding(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_encode)\n",
        "print(\"Negative:\", neg_encode)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n",
            "Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45a9tZ0Tpfnq",
        "colab_type": "code",
        "outputId": "460fd4da-e24a-4c13-959e-808d2a6d85f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE3ssX5StQ7a",
        "colab_type": "code",
        "outputId": "04442e9b-2d25-424c-a936-63531384298e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Lets look at one review\n",
        "#train_data[0]\n",
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(218, 189)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeS4lsnVtUSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pws4hlYgklq",
        "colab_type": "code",
        "outputId": "3e4b0592-a9c5-4790-fc0a-6870d7908d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "train_data[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     1,   194,\n",
              "        1153,   194,  8255,    78,   228,     5,     6,  1463,  4369,\n",
              "        5012,   134,    26,     4,   715,     8,   118,  1634,    14,\n",
              "         394,    20,    13,   119,   954,   189,   102,     5,   207,\n",
              "         110,  3103,    21,    14,    69,   188,     8,    30,    23,\n",
              "           7,     4,   249,   126,    93,     4,   114,     9,  2300,\n",
              "        1523,     5,   647,     4,   116,     9,    35,  8163,     4,\n",
              "         229,     9,   340,  1322,     4,   118,     9,     4,   130,\n",
              "        4901,    19,     4,  1002,     5,    89,    29,   952,    46,\n",
              "          37,     4,   455,     9,    45,    43,    38,  1543,  1905,\n",
              "         398,     4,  1649,    26,  6853,     5,   163,    11,  3215,\n",
              "       10156,     4,  1153,     9,   194,   775,     7,  8255, 11596,\n",
              "         349,  2637,   148,   605, 15358,  8003,    15,   123,   125,\n",
              "          68, 23141,  6853,    15,   349,   165,  4362,    98,     5,\n",
              "           4,   228,     9,    43, 36893,  1157,    15,   299,   120,\n",
              "           5,   120,   174,    11,   220,   175,   136,    50,     9,\n",
              "        4373,   228,  8255,     5, 25249,   656,   245,  2350,     5,\n",
              "           4,  9837,   131,   152,   491,    18, 46151,    32,  7464,\n",
              "        1212,    14,     9,     6,   371,    78,    22,   625,    64,\n",
              "        1382,     9,     8,   168,   145,    23,     4,  1690,    15,\n",
              "          16,     4,  1355,     5,    28,     6,    52,   154,   462,\n",
              "          33,    89,    78,   285,    16,   145,    95], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arZ48WXYgYcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRbleaGdjC6D",
        "colab_type": "code",
        "outputId": "9ad57386-89cf-489d-bff0-11d7c3b5b764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZfQJ5LtjFAt",
        "colab_type": "code",
        "outputId": "3b7f3469-36d1-44f2-f265-c98ea66bdd85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.4307 - acc: 0.8011 - val_loss: 0.3865 - val_acc: 0.8356\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.2447 - acc: 0.9068 - val_loss: 0.3067 - val_acc: 0.8760\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.1824 - acc: 0.9345 - val_loss: 0.3690 - val_acc: 0.8544\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.1547 - acc: 0.9456 - val_loss: 0.3145 - val_acc: 0.8836\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.1293 - acc: 0.9546 - val_loss: 0.2908 - val_acc: 0.8916\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.1131 - acc: 0.9611 - val_loss: 0.3511 - val_acc: 0.8816\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.0975 - acc: 0.9677 - val_loss: 0.3419 - val_acc: 0.8802\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.0882 - acc: 0.9705 - val_loss: 0.3707 - val_acc: 0.8858\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.0785 - acc: 0.9748 - val_loss: 0.4413 - val_acc: 0.8702\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.0702 - acc: 0.9773 - val_loss: 0.4425 - val_acc: 0.8802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO7ieY2Vjftu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkNNQoywneGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNVe3A5_nhdF",
        "colab_type": "code",
        "outputId": "7b8d0a18-c220-497d-94c9-6778a37bfd14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "len(word_index), word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88584,\n",
              " {'fawn': 34701,\n",
              "  'tsukino': 52006,\n",
              "  'nunnery': 52007,\n",
              "  'sonja': 16816,\n",
              "  'vani': 63951,\n",
              "  'woods': 1408,\n",
              "  'spiders': 16115,\n",
              "  'hanging': 2345,\n",
              "  'woody': 2289,\n",
              "  'trawling': 52008,\n",
              "  \"hold's\": 52009,\n",
              "  'comically': 11307,\n",
              "  'localized': 40830,\n",
              "  'disobeying': 30568,\n",
              "  \"'royale\": 52010,\n",
              "  \"harpo's\": 40831,\n",
              "  'canet': 52011,\n",
              "  'aileen': 19313,\n",
              "  'acurately': 52012,\n",
              "  \"diplomat's\": 52013,\n",
              "  'rickman': 25242,\n",
              "  'arranged': 6746,\n",
              "  'rumbustious': 52014,\n",
              "  'familiarness': 52015,\n",
              "  \"spider'\": 52016,\n",
              "  'hahahah': 68804,\n",
              "  \"wood'\": 52017,\n",
              "  'transvestism': 40833,\n",
              "  \"hangin'\": 34702,\n",
              "  'bringing': 2338,\n",
              "  'seamier': 40834,\n",
              "  'wooded': 34703,\n",
              "  'bravora': 52018,\n",
              "  'grueling': 16817,\n",
              "  'wooden': 1636,\n",
              "  'wednesday': 16818,\n",
              "  \"'prix\": 52019,\n",
              "  'altagracia': 34704,\n",
              "  'circuitry': 52020,\n",
              "  'crotch': 11585,\n",
              "  'busybody': 57766,\n",
              "  \"tart'n'tangy\": 52021,\n",
              "  'burgade': 14129,\n",
              "  'thrace': 52023,\n",
              "  \"tom's\": 11038,\n",
              "  'snuggles': 52025,\n",
              "  'francesco': 29114,\n",
              "  'complainers': 52027,\n",
              "  'templarios': 52125,\n",
              "  '272': 40835,\n",
              "  '273': 52028,\n",
              "  'zaniacs': 52130,\n",
              "  '275': 34706,\n",
              "  'consenting': 27631,\n",
              "  'snuggled': 40836,\n",
              "  'inanimate': 15492,\n",
              "  'uality': 52030,\n",
              "  'bronte': 11926,\n",
              "  'errors': 4010,\n",
              "  'dialogs': 3230,\n",
              "  \"yomada's\": 52031,\n",
              "  \"madman's\": 34707,\n",
              "  'dialoge': 30585,\n",
              "  'usenet': 52033,\n",
              "  'videodrome': 40837,\n",
              "  \"kid'\": 26338,\n",
              "  'pawed': 52034,\n",
              "  \"'girlfriend'\": 30569,\n",
              "  \"'pleasure\": 52035,\n",
              "  \"'reloaded'\": 52036,\n",
              "  \"kazakos'\": 40839,\n",
              "  'rocque': 52037,\n",
              "  'mailings': 52038,\n",
              "  'brainwashed': 11927,\n",
              "  'mcanally': 16819,\n",
              "  \"tom''\": 52039,\n",
              "  'kurupt': 25243,\n",
              "  'affiliated': 21905,\n",
              "  'babaganoosh': 52040,\n",
              "  \"noe's\": 40840,\n",
              "  'quart': 40841,\n",
              "  'kids': 359,\n",
              "  'uplifting': 5034,\n",
              "  'controversy': 7093,\n",
              "  'kida': 21906,\n",
              "  'kidd': 23379,\n",
              "  \"error'\": 52041,\n",
              "  'neurologist': 52042,\n",
              "  'spotty': 18510,\n",
              "  'cobblers': 30570,\n",
              "  'projection': 9878,\n",
              "  'fastforwarding': 40842,\n",
              "  'sters': 52043,\n",
              "  \"eggar's\": 52044,\n",
              "  'etherything': 52045,\n",
              "  'gateshead': 40843,\n",
              "  'airball': 34708,\n",
              "  'unsinkable': 25244,\n",
              "  'stern': 7180,\n",
              "  \"cervi's\": 52046,\n",
              "  'dnd': 40844,\n",
              "  'dna': 11586,\n",
              "  'insecurity': 20598,\n",
              "  \"'reboot'\": 52047,\n",
              "  'trelkovsky': 11037,\n",
              "  'jaekel': 52048,\n",
              "  'sidebars': 52049,\n",
              "  \"sforza's\": 52050,\n",
              "  'distortions': 17633,\n",
              "  'mutinies': 52051,\n",
              "  'sermons': 30602,\n",
              "  '7ft': 40846,\n",
              "  'boobage': 52052,\n",
              "  \"o'bannon's\": 52053,\n",
              "  'populations': 23380,\n",
              "  'chulak': 52054,\n",
              "  'mesmerize': 27633,\n",
              "  'quinnell': 52055,\n",
              "  'yahoo': 10307,\n",
              "  'meteorologist': 52057,\n",
              "  'beswick': 42577,\n",
              "  'boorman': 15493,\n",
              "  'voicework': 40847,\n",
              "  \"ster'\": 52058,\n",
              "  'blustering': 22922,\n",
              "  'hj': 52059,\n",
              "  'intake': 27634,\n",
              "  'morally': 5621,\n",
              "  'jumbling': 40849,\n",
              "  'bowersock': 52060,\n",
              "  \"'porky's'\": 52061,\n",
              "  'gershon': 16821,\n",
              "  'ludicrosity': 40850,\n",
              "  'coprophilia': 52062,\n",
              "  'expressively': 40851,\n",
              "  \"india's\": 19500,\n",
              "  \"post's\": 34710,\n",
              "  'wana': 52063,\n",
              "  'wang': 5283,\n",
              "  'wand': 30571,\n",
              "  'wane': 25245,\n",
              "  'edgeways': 52321,\n",
              "  'titanium': 34711,\n",
              "  'pinta': 40852,\n",
              "  'want': 178,\n",
              "  'pinto': 30572,\n",
              "  'whoopdedoodles': 52065,\n",
              "  'tchaikovsky': 21908,\n",
              "  'travel': 2103,\n",
              "  \"'victory'\": 52066,\n",
              "  'copious': 11928,\n",
              "  'gouge': 22433,\n",
              "  \"chapters'\": 52067,\n",
              "  'barbra': 6702,\n",
              "  'uselessness': 30573,\n",
              "  \"wan'\": 52068,\n",
              "  'assimilated': 27635,\n",
              "  'petiot': 16116,\n",
              "  'most\\x85and': 52069,\n",
              "  'dinosaurs': 3930,\n",
              "  'wrong': 352,\n",
              "  'seda': 52070,\n",
              "  'stollen': 52071,\n",
              "  'sentencing': 34712,\n",
              "  'ouroboros': 40853,\n",
              "  'assimilates': 40854,\n",
              "  'colorfully': 40855,\n",
              "  'glenne': 27636,\n",
              "  'dongen': 52072,\n",
              "  'subplots': 4760,\n",
              "  'kiloton': 52073,\n",
              "  'chandon': 23381,\n",
              "  \"effect'\": 34713,\n",
              "  'snugly': 27637,\n",
              "  'kuei': 40856,\n",
              "  'welcomed': 9092,\n",
              "  'dishonor': 30071,\n",
              "  'concurrence': 52075,\n",
              "  'stoicism': 23382,\n",
              "  \"guys'\": 14896,\n",
              "  \"beroemd'\": 52077,\n",
              "  'butcher': 6703,\n",
              "  \"melfi's\": 40857,\n",
              "  'aargh': 30623,\n",
              "  'playhouse': 20599,\n",
              "  'wickedly': 11308,\n",
              "  'fit': 1180,\n",
              "  'labratory': 52078,\n",
              "  'lifeline': 40859,\n",
              "  'screaming': 1927,\n",
              "  'fix': 4287,\n",
              "  'cineliterate': 52079,\n",
              "  'fic': 52080,\n",
              "  'fia': 52081,\n",
              "  'fig': 34714,\n",
              "  'fmvs': 52082,\n",
              "  'fie': 52083,\n",
              "  'reentered': 52084,\n",
              "  'fin': 30574,\n",
              "  'doctresses': 52085,\n",
              "  'fil': 52086,\n",
              "  'zucker': 12606,\n",
              "  'ached': 31931,\n",
              "  'counsil': 52088,\n",
              "  'paterfamilias': 52089,\n",
              "  'songwriter': 13885,\n",
              "  'shivam': 34715,\n",
              "  'hurting': 9654,\n",
              "  'effects': 299,\n",
              "  'slauther': 52090,\n",
              "  \"'flame'\": 52091,\n",
              "  'sommerset': 52092,\n",
              "  'interwhined': 52093,\n",
              "  'whacking': 27638,\n",
              "  'bartok': 52094,\n",
              "  'barton': 8775,\n",
              "  'frewer': 21909,\n",
              "  \"fi'\": 52095,\n",
              "  'ingrid': 6192,\n",
              "  'stribor': 30575,\n",
              "  'approporiately': 52096,\n",
              "  'wobblyhand': 52097,\n",
              "  'tantalisingly': 52098,\n",
              "  'ankylosaurus': 52099,\n",
              "  'parasites': 17634,\n",
              "  'childen': 52100,\n",
              "  \"jenkins'\": 52101,\n",
              "  'metafiction': 52102,\n",
              "  'golem': 17635,\n",
              "  'indiscretion': 40860,\n",
              "  \"reeves'\": 23383,\n",
              "  \"inamorata's\": 57781,\n",
              "  'brittannica': 52104,\n",
              "  'adapt': 7916,\n",
              "  \"russo's\": 30576,\n",
              "  'guitarists': 48246,\n",
              "  'abbott': 10553,\n",
              "  'abbots': 40861,\n",
              "  'lanisha': 17649,\n",
              "  'magickal': 40863,\n",
              "  'mattter': 52105,\n",
              "  \"'willy\": 52106,\n",
              "  'pumpkins': 34716,\n",
              "  'stuntpeople': 52107,\n",
              "  'estimate': 30577,\n",
              "  'ugghhh': 40864,\n",
              "  'gameplay': 11309,\n",
              "  \"wern't\": 52108,\n",
              "  \"n'sync\": 40865,\n",
              "  'sickeningly': 16117,\n",
              "  'chiara': 40866,\n",
              "  'disturbed': 4011,\n",
              "  'portmanteau': 40867,\n",
              "  'ineffectively': 52109,\n",
              "  \"duchonvey's\": 82143,\n",
              "  \"nasty'\": 37519,\n",
              "  'purpose': 1285,\n",
              "  'lazers': 52112,\n",
              "  'lightened': 28105,\n",
              "  'kaliganj': 52113,\n",
              "  'popularism': 52114,\n",
              "  \"damme's\": 18511,\n",
              "  'stylistics': 30578,\n",
              "  'mindgaming': 52115,\n",
              "  'spoilerish': 46449,\n",
              "  \"'corny'\": 52117,\n",
              "  'boerner': 34718,\n",
              "  'olds': 6792,\n",
              "  'bakelite': 52118,\n",
              "  'renovated': 27639,\n",
              "  'forrester': 27640,\n",
              "  \"lumiere's\": 52119,\n",
              "  'gaskets': 52024,\n",
              "  'needed': 884,\n",
              "  'smight': 34719,\n",
              "  'master': 1297,\n",
              "  \"edie's\": 25905,\n",
              "  'seeber': 40868,\n",
              "  'hiya': 52120,\n",
              "  'fuzziness': 52121,\n",
              "  'genesis': 14897,\n",
              "  'rewards': 12607,\n",
              "  'enthrall': 30579,\n",
              "  \"'about\": 40869,\n",
              "  \"recollection's\": 52122,\n",
              "  'mutilated': 11039,\n",
              "  'fatherlands': 52123,\n",
              "  \"fischer's\": 52124,\n",
              "  'positively': 5399,\n",
              "  '270': 34705,\n",
              "  'ahmed': 34720,\n",
              "  'zatoichi': 9836,\n",
              "  'bannister': 13886,\n",
              "  'anniversaries': 52127,\n",
              "  \"helm's\": 30580,\n",
              "  \"'work'\": 52128,\n",
              "  'exclaimed': 34721,\n",
              "  \"'unfunny'\": 52129,\n",
              "  '274': 52029,\n",
              "  'feeling': 544,\n",
              "  \"wanda's\": 52131,\n",
              "  'dolan': 33266,\n",
              "  '278': 52133,\n",
              "  'peacoat': 52134,\n",
              "  'brawny': 40870,\n",
              "  'mishra': 40871,\n",
              "  'worlders': 40872,\n",
              "  'protags': 52135,\n",
              "  'skullcap': 52136,\n",
              "  'dastagir': 57596,\n",
              "  'affairs': 5622,\n",
              "  'wholesome': 7799,\n",
              "  'hymen': 52137,\n",
              "  'paramedics': 25246,\n",
              "  'unpersons': 52138,\n",
              "  'heavyarms': 52139,\n",
              "  'affaire': 52140,\n",
              "  'coulisses': 52141,\n",
              "  'hymer': 40873,\n",
              "  'kremlin': 52142,\n",
              "  'shipments': 30581,\n",
              "  'pixilated': 52143,\n",
              "  \"'00s\": 30582,\n",
              "  'diminishing': 18512,\n",
              "  'cinematic': 1357,\n",
              "  'resonates': 14898,\n",
              "  'simplify': 40874,\n",
              "  \"nature'\": 40875,\n",
              "  'temptresses': 40876,\n",
              "  'reverence': 16822,\n",
              "  'resonated': 19502,\n",
              "  'dailey': 34722,\n",
              "  '2\\x85': 52144,\n",
              "  'treize': 27641,\n",
              "  'majo': 52145,\n",
              "  'kiya': 21910,\n",
              "  'woolnough': 52146,\n",
              "  'thanatos': 39797,\n",
              "  'sandoval': 35731,\n",
              "  'dorama': 40879,\n",
              "  \"o'shaughnessy\": 52147,\n",
              "  'tech': 4988,\n",
              "  'fugitives': 32018,\n",
              "  'teck': 30583,\n",
              "  \"'e'\": 76125,\n",
              "  'doesn’t': 40881,\n",
              "  'purged': 52149,\n",
              "  'saying': 657,\n",
              "  \"martians'\": 41095,\n",
              "  'norliss': 23418,\n",
              "  'dickey': 27642,\n",
              "  'dicker': 52152,\n",
              "  \"'sependipity\": 52153,\n",
              "  'padded': 8422,\n",
              "  'ordell': 57792,\n",
              "  \"sturges'\": 40882,\n",
              "  'independentcritics': 52154,\n",
              "  'tempted': 5745,\n",
              "  \"atkinson's\": 34724,\n",
              "  'hounded': 25247,\n",
              "  'apace': 52155,\n",
              "  'clicked': 15494,\n",
              "  \"'humor'\": 30584,\n",
              "  \"martino's\": 17177,\n",
              "  \"'supporting\": 52156,\n",
              "  'warmongering': 52032,\n",
              "  \"zemeckis's\": 34725,\n",
              "  'lube': 21911,\n",
              "  'shocky': 52157,\n",
              "  'plate': 7476,\n",
              "  'plata': 40883,\n",
              "  'sturgess': 40884,\n",
              "  \"nerds'\": 40885,\n",
              "  'plato': 20600,\n",
              "  'plath': 34726,\n",
              "  'platt': 40886,\n",
              "  'mcnab': 52159,\n",
              "  'clumsiness': 27643,\n",
              "  'altogether': 3899,\n",
              "  'massacring': 42584,\n",
              "  'bicenntinial': 52160,\n",
              "  'skaal': 40887,\n",
              "  'droning': 14360,\n",
              "  'lds': 8776,\n",
              "  'jaguar': 21912,\n",
              "  \"cale's\": 34727,\n",
              "  'nicely': 1777,\n",
              "  'mummy': 4588,\n",
              "  \"lot's\": 18513,\n",
              "  'patch': 10086,\n",
              "  'kerkhof': 50202,\n",
              "  \"leader's\": 52161,\n",
              "  \"'movie\": 27644,\n",
              "  'uncomfirmed': 52162,\n",
              "  'heirloom': 40888,\n",
              "  'wrangle': 47360,\n",
              "  'emotion\\x85': 52163,\n",
              "  \"'stargate'\": 52164,\n",
              "  'pinoy': 40889,\n",
              "  'conchatta': 40890,\n",
              "  'broeke': 41128,\n",
              "  'advisedly': 40891,\n",
              "  \"barker's\": 17636,\n",
              "  'descours': 52166,\n",
              "  'lots': 772,\n",
              "  'lotr': 9259,\n",
              "  'irs': 9879,\n",
              "  'lott': 52167,\n",
              "  'xvi': 40892,\n",
              "  'irk': 34728,\n",
              "  'irl': 52168,\n",
              "  'ira': 6887,\n",
              "  'belzer': 21913,\n",
              "  'irc': 52169,\n",
              "  'ire': 27645,\n",
              "  'requisites': 40893,\n",
              "  'discipline': 7693,\n",
              "  'lyoko': 52961,\n",
              "  'extend': 11310,\n",
              "  'nature': 873,\n",
              "  \"'dickie'\": 52170,\n",
              "  'optimist': 40894,\n",
              "  'lapping': 30586,\n",
              "  'superficial': 3900,\n",
              "  'vestment': 52171,\n",
              "  'extent': 2823,\n",
              "  'tendons': 52172,\n",
              "  \"heller's\": 52173,\n",
              "  'quagmires': 52174,\n",
              "  'miyako': 52175,\n",
              "  'moocow': 20601,\n",
              "  \"coles'\": 52176,\n",
              "  'lookit': 40895,\n",
              "  'ravenously': 52177,\n",
              "  'levitating': 40896,\n",
              "  'perfunctorily': 52178,\n",
              "  'lookin': 30587,\n",
              "  \"lot'\": 40898,\n",
              "  'lookie': 52179,\n",
              "  'fearlessly': 34870,\n",
              "  'libyan': 52181,\n",
              "  'fondles': 40899,\n",
              "  'gopher': 35714,\n",
              "  'wearying': 40901,\n",
              "  \"nz's\": 52182,\n",
              "  'minuses': 27646,\n",
              "  'puposelessly': 52183,\n",
              "  'shandling': 52184,\n",
              "  'decapitates': 31268,\n",
              "  'humming': 11929,\n",
              "  \"'nother\": 40902,\n",
              "  'smackdown': 21914,\n",
              "  'underdone': 30588,\n",
              "  'frf': 40903,\n",
              "  'triviality': 52185,\n",
              "  'fro': 25248,\n",
              "  'bothers': 8777,\n",
              "  \"'kensington\": 52186,\n",
              "  'much': 73,\n",
              "  'muco': 34730,\n",
              "  'wiseguy': 22615,\n",
              "  \"richie's\": 27648,\n",
              "  'tonino': 40904,\n",
              "  'unleavened': 52187,\n",
              "  'fry': 11587,\n",
              "  \"'tv'\": 40905,\n",
              "  'toning': 40906,\n",
              "  'obese': 14361,\n",
              "  'sensationalized': 30589,\n",
              "  'spiv': 40907,\n",
              "  'spit': 6259,\n",
              "  'arkin': 7364,\n",
              "  'charleton': 21915,\n",
              "  'jeon': 16823,\n",
              "  'boardroom': 21916,\n",
              "  'doubts': 4989,\n",
              "  'spin': 3084,\n",
              "  'hepo': 53083,\n",
              "  'wildcat': 27649,\n",
              "  'venoms': 10584,\n",
              "  'misconstrues': 52191,\n",
              "  'mesmerising': 18514,\n",
              "  'misconstrued': 40908,\n",
              "  'rescinds': 52192,\n",
              "  'prostrate': 52193,\n",
              "  'majid': 40909,\n",
              "  'climbed': 16479,\n",
              "  'canoeing': 34731,\n",
              "  'majin': 52195,\n",
              "  'animie': 57804,\n",
              "  'sylke': 40910,\n",
              "  'conditioned': 14899,\n",
              "  'waddell': 40911,\n",
              "  '3\\x85': 52196,\n",
              "  'hyperdrive': 41188,\n",
              "  'conditioner': 34732,\n",
              "  'bricklayer': 53153,\n",
              "  'hong': 2576,\n",
              "  'memoriam': 52198,\n",
              "  'inventively': 30592,\n",
              "  \"levant's\": 25249,\n",
              "  'portobello': 20638,\n",
              "  'remand': 52200,\n",
              "  'mummified': 19504,\n",
              "  'honk': 27650,\n",
              "  'spews': 19505,\n",
              "  'visitations': 40912,\n",
              "  'mummifies': 52201,\n",
              "  'cavanaugh': 25250,\n",
              "  'zeon': 23385,\n",
              "  \"jungle's\": 40913,\n",
              "  'viertel': 34733,\n",
              "  'frenchmen': 27651,\n",
              "  'torpedoes': 52202,\n",
              "  'schlessinger': 52203,\n",
              "  'torpedoed': 34734,\n",
              "  'blister': 69876,\n",
              "  'cinefest': 52204,\n",
              "  'furlough': 34735,\n",
              "  'mainsequence': 52205,\n",
              "  'mentors': 40914,\n",
              "  'academic': 9094,\n",
              "  'stillness': 20602,\n",
              "  'academia': 40915,\n",
              "  'lonelier': 52206,\n",
              "  'nibby': 52207,\n",
              "  \"losers'\": 52208,\n",
              "  'cineastes': 40916,\n",
              "  'corporate': 4449,\n",
              "  'massaging': 40917,\n",
              "  'bellow': 30593,\n",
              "  'absurdities': 19506,\n",
              "  'expetations': 53241,\n",
              "  'nyfiken': 40918,\n",
              "  'mehras': 75638,\n",
              "  'lasse': 52209,\n",
              "  'visability': 52210,\n",
              "  'militarily': 33946,\n",
              "  \"elder'\": 52211,\n",
              "  'gainsbourg': 19023,\n",
              "  'hah': 20603,\n",
              "  'hai': 13420,\n",
              "  'haj': 34736,\n",
              "  'hak': 25251,\n",
              "  'hal': 4311,\n",
              "  'ham': 4892,\n",
              "  'duffer': 53259,\n",
              "  'haa': 52213,\n",
              "  'had': 66,\n",
              "  'advancement': 11930,\n",
              "  'hag': 16825,\n",
              "  \"hand'\": 25252,\n",
              "  'hay': 13421,\n",
              "  'mcnamara': 20604,\n",
              "  \"mozart's\": 52214,\n",
              "  'duffel': 30731,\n",
              "  'haq': 30594,\n",
              "  'har': 13887,\n",
              "  'has': 44,\n",
              "  'hat': 2401,\n",
              "  'hav': 40919,\n",
              "  'haw': 30595,\n",
              "  'figtings': 52215,\n",
              "  'elders': 15495,\n",
              "  'underpanted': 52216,\n",
              "  'pninson': 52217,\n",
              "  'unequivocally': 27652,\n",
              "  \"barbara's\": 23673,\n",
              "  \"bello'\": 52219,\n",
              "  'indicative': 12997,\n",
              "  'yawnfest': 40920,\n",
              "  'hexploitation': 52220,\n",
              "  \"loder's\": 52221,\n",
              "  'sleuthing': 27653,\n",
              "  \"justin's\": 32622,\n",
              "  \"'ball\": 52222,\n",
              "  \"'summer\": 52223,\n",
              "  \"'demons'\": 34935,\n",
              "  \"mormon's\": 52225,\n",
              "  \"laughton's\": 34737,\n",
              "  'debell': 52226,\n",
              "  'shipyard': 39724,\n",
              "  'unabashedly': 30597,\n",
              "  'disks': 40401,\n",
              "  'crowd': 2290,\n",
              "  'crowe': 10087,\n",
              "  \"vancouver's\": 56434,\n",
              "  'mosques': 34738,\n",
              "  'crown': 6627,\n",
              "  'culpas': 52227,\n",
              "  'crows': 27654,\n",
              "  'surrell': 53344,\n",
              "  'flowless': 52229,\n",
              "  'sheirk': 52230,\n",
              "  \"'three\": 40923,\n",
              "  \"peterson'\": 52231,\n",
              "  'ooverall': 52232,\n",
              "  'perchance': 40924,\n",
              "  'bottom': 1321,\n",
              "  'chabert': 53363,\n",
              "  'sneha': 52233,\n",
              "  'inhuman': 13888,\n",
              "  'ichii': 52234,\n",
              "  'ursla': 52235,\n",
              "  'completly': 30598,\n",
              "  'moviedom': 40925,\n",
              "  'raddick': 52236,\n",
              "  'brundage': 51995,\n",
              "  'brigades': 40926,\n",
              "  'starring': 1181,\n",
              "  \"'goal'\": 52237,\n",
              "  'caskets': 52238,\n",
              "  'willcock': 52239,\n",
              "  \"threesome's\": 52240,\n",
              "  \"mosque'\": 52241,\n",
              "  \"cover's\": 52242,\n",
              "  'spaceships': 17637,\n",
              "  'anomalous': 40927,\n",
              "  'ptsd': 27655,\n",
              "  'shirdan': 52243,\n",
              "  'obscenity': 21962,\n",
              "  'lemmings': 30599,\n",
              "  'duccio': 30600,\n",
              "  \"levene's\": 52244,\n",
              "  \"'gorby'\": 52245,\n",
              "  \"teenager's\": 25255,\n",
              "  'marshall': 5340,\n",
              "  'honeymoon': 9095,\n",
              "  'shoots': 3231,\n",
              "  'despised': 12258,\n",
              "  'okabasho': 52246,\n",
              "  'fabric': 8289,\n",
              "  'cannavale': 18515,\n",
              "  'raped': 3537,\n",
              "  \"tutt's\": 52247,\n",
              "  'grasping': 17638,\n",
              "  'despises': 18516,\n",
              "  \"thief's\": 40928,\n",
              "  'rapes': 8926,\n",
              "  'raper': 52248,\n",
              "  \"eyre'\": 27656,\n",
              "  'walchek': 52249,\n",
              "  \"elmo's\": 23386,\n",
              "  'perfumes': 40929,\n",
              "  'spurting': 21918,\n",
              "  \"exposition'\\x85\": 52250,\n",
              "  'denoting': 52251,\n",
              "  'thesaurus': 34740,\n",
              "  \"shoot'\": 40930,\n",
              "  'bonejack': 49759,\n",
              "  'simpsonian': 52253,\n",
              "  'hebetude': 30601,\n",
              "  \"hallow's\": 34741,\n",
              "  'desperation\\x85': 52254,\n",
              "  'incinerator': 34742,\n",
              "  'congratulations': 10308,\n",
              "  'humbled': 52255,\n",
              "  \"else's\": 5924,\n",
              "  'trelkovski': 40845,\n",
              "  \"rape'\": 52256,\n",
              "  \"'chapters'\": 59386,\n",
              "  '1600s': 52257,\n",
              "  'martian': 7253,\n",
              "  'nicest': 25256,\n",
              "  'eyred': 52259,\n",
              "  'passenger': 9457,\n",
              "  'disgrace': 6041,\n",
              "  'moderne': 52260,\n",
              "  'barrymore': 5120,\n",
              "  'yankovich': 52261,\n",
              "  'moderns': 40931,\n",
              "  'studliest': 52262,\n",
              "  'bedsheet': 52263,\n",
              "  'decapitation': 14900,\n",
              "  'slurring': 52264,\n",
              "  \"'nunsploitation'\": 52265,\n",
              "  \"'character'\": 34743,\n",
              "  'cambodia': 9880,\n",
              "  'rebelious': 52266,\n",
              "  'pasadena': 27657,\n",
              "  'crowne': 40932,\n",
              "  \"'bedchamber\": 52267,\n",
              "  'conjectural': 52268,\n",
              "  'appologize': 52269,\n",
              "  'halfassing': 52270,\n",
              "  'paycheque': 57816,\n",
              "  'palms': 20606,\n",
              "  \"'islands\": 52271,\n",
              "  'hawked': 40933,\n",
              "  'palme': 21919,\n",
              "  'conservatively': 40934,\n",
              "  'larp': 64007,\n",
              "  'palma': 5558,\n",
              "  'smelling': 21920,\n",
              "  'aragorn': 12998,\n",
              "  'hawker': 52272,\n",
              "  'hawkes': 52273,\n",
              "  'explosions': 3975,\n",
              "  'loren': 8059,\n",
              "  \"pyle's\": 52274,\n",
              "  'shootout': 6704,\n",
              "  \"mike's\": 18517,\n",
              "  \"driscoll's\": 52275,\n",
              "  'cogsworth': 40935,\n",
              "  \"britian's\": 52276,\n",
              "  'childs': 34744,\n",
              "  \"portrait's\": 52277,\n",
              "  'chain': 3626,\n",
              "  'whoever': 2497,\n",
              "  'puttered': 52278,\n",
              "  'childe': 52279,\n",
              "  'maywether': 52280,\n",
              "  'chair': 3036,\n",
              "  \"rance's\": 52281,\n",
              "  'machu': 34745,\n",
              "  'ballet': 4517,\n",
              "  'grapples': 34746,\n",
              "  'summerize': 76152,\n",
              "  'freelance': 30603,\n",
              "  \"andrea's\": 52283,\n",
              "  '\\x91very': 52284,\n",
              "  'coolidge': 45879,\n",
              "  'mache': 18518,\n",
              "  'balled': 52285,\n",
              "  'grappled': 40937,\n",
              "  'macha': 18519,\n",
              "  'underlining': 21921,\n",
              "  'macho': 5623,\n",
              "  'oversight': 19507,\n",
              "  'machi': 25257,\n",
              "  'verbally': 11311,\n",
              "  'tenacious': 21922,\n",
              "  'windshields': 40938,\n",
              "  'paychecks': 18557,\n",
              "  'jerk': 3396,\n",
              "  \"good'\": 11931,\n",
              "  'prancer': 34748,\n",
              "  'prances': 21923,\n",
              "  'olympus': 52286,\n",
              "  'lark': 21924,\n",
              "  'embark': 10785,\n",
              "  'gloomy': 7365,\n",
              "  'jehaan': 52287,\n",
              "  'turaqui': 52288,\n",
              "  \"child'\": 20607,\n",
              "  'locked': 2894,\n",
              "  'pranced': 52289,\n",
              "  'exact': 2588,\n",
              "  'unattuned': 52290,\n",
              "  'minute': 783,\n",
              "  'skewed': 16118,\n",
              "  'hodgins': 40940,\n",
              "  'skewer': 34749,\n",
              "  'think\\x85': 52291,\n",
              "  'rosenstein': 38765,\n",
              "  'helmit': 52292,\n",
              "  'wrestlemanias': 34750,\n",
              "  'hindered': 16826,\n",
              "  \"martha's\": 30604,\n",
              "  'cheree': 52293,\n",
              "  \"pluckin'\": 52294,\n",
              "  'ogles': 40941,\n",
              "  'heavyweight': 11932,\n",
              "  'aada': 82190,\n",
              "  'chopping': 11312,\n",
              "  'strongboy': 61534,\n",
              "  'hegemonic': 41342,\n",
              "  'adorns': 40942,\n",
              "  'xxth': 41346,\n",
              "  'nobuhiro': 34751,\n",
              "  'capitães': 52298,\n",
              "  'kavogianni': 52299,\n",
              "  'antwerp': 13422,\n",
              "  'celebrated': 6538,\n",
              "  'roarke': 52300,\n",
              "  'baggins': 40943,\n",
              "  'cheeseburgers': 31270,\n",
              "  'matras': 52301,\n",
              "  \"nineties'\": 52302,\n",
              "  \"'craig'\": 52303,\n",
              "  'celebrates': 12999,\n",
              "  'unintentionally': 3383,\n",
              "  'drafted': 14362,\n",
              "  'climby': 52304,\n",
              "  '303': 52305,\n",
              "  'oldies': 18520,\n",
              "  'climbs': 9096,\n",
              "  'honour': 9655,\n",
              "  'plucking': 34752,\n",
              "  '305': 30074,\n",
              "  'address': 5514,\n",
              "  'menjou': 40944,\n",
              "  \"'freak'\": 42592,\n",
              "  'dwindling': 19508,\n",
              "  'benson': 9458,\n",
              "  'white’s': 52307,\n",
              "  'shamelessness': 40945,\n",
              "  'impacted': 21925,\n",
              "  'upatz': 52308,\n",
              "  'cusack': 3840,\n",
              "  \"flavia's\": 37567,\n",
              "  'effette': 52309,\n",
              "  'influx': 34753,\n",
              "  'boooooooo': 52310,\n",
              "  'dimitrova': 52311,\n",
              "  'houseman': 13423,\n",
              "  'bigas': 25259,\n",
              "  'boylen': 52312,\n",
              "  'phillipenes': 52313,\n",
              "  'fakery': 40946,\n",
              "  \"grandpa's\": 27658,\n",
              "  'darnell': 27659,\n",
              "  'undergone': 19509,\n",
              "  'handbags': 52315,\n",
              "  'perished': 21926,\n",
              "  'pooped': 37778,\n",
              "  'vigour': 27660,\n",
              "  'opposed': 3627,\n",
              "  'etude': 52316,\n",
              "  \"caine's\": 11799,\n",
              "  'doozers': 52317,\n",
              "  'photojournals': 34754,\n",
              "  'perishes': 52318,\n",
              "  'constrains': 34755,\n",
              "  'migenes': 40948,\n",
              "  'consoled': 30605,\n",
              "  'alastair': 16827,\n",
              "  'wvs': 52319,\n",
              "  'ooooooh': 52320,\n",
              "  'approving': 34756,\n",
              "  'consoles': 40949,\n",
              "  'disparagement': 52064,\n",
              "  'futureistic': 52322,\n",
              "  'rebounding': 52323,\n",
              "  \"'date\": 52324,\n",
              "  'gregoire': 52325,\n",
              "  'rutherford': 21927,\n",
              "  'americanised': 34757,\n",
              "  'novikov': 82196,\n",
              "  'following': 1042,\n",
              "  'munroe': 34758,\n",
              "  \"morita'\": 52326,\n",
              "  'christenssen': 52327,\n",
              "  'oatmeal': 23106,\n",
              "  'fossey': 25260,\n",
              "  'livered': 40950,\n",
              "  'listens': 13000,\n",
              "  \"'marci\": 76164,\n",
              "  \"otis's\": 52330,\n",
              "  'thanking': 23387,\n",
              "  'maude': 16019,\n",
              "  'extensions': 34759,\n",
              "  'ameteurish': 52332,\n",
              "  \"commender's\": 52333,\n",
              "  'agricultural': 27661,\n",
              "  'convincingly': 4518,\n",
              "  'fueled': 17639,\n",
              "  'mahattan': 54014,\n",
              "  \"paris's\": 40952,\n",
              "  'vulkan': 52336,\n",
              "  'stapes': 52337,\n",
              "  'odysessy': 52338,\n",
              "  'harmon': 12259,\n",
              "  'surfing': 4252,\n",
              "  'halloran': 23494,\n",
              "  'unbelieveably': 49580,\n",
              "  \"'offed'\": 52339,\n",
              "  'quadrant': 30607,\n",
              "  'inhabiting': 19510,\n",
              "  'nebbish': 34760,\n",
              "  'forebears': 40953,\n",
              "  'skirmish': 34761,\n",
              "  'ocassionally': 52340,\n",
              "  \"'resist\": 52341,\n",
              "  'impactful': 21928,\n",
              "  'spicier': 52342,\n",
              "  'touristy': 40954,\n",
              "  \"'football'\": 52343,\n",
              "  'webpage': 40955,\n",
              "  'exurbia': 52345,\n",
              "  'jucier': 52346,\n",
              "  'professors': 14901,\n",
              "  'structuring': 34762,\n",
              "  'jig': 30608,\n",
              "  'overlord': 40956,\n",
              "  'disconnect': 25261,\n",
              "  'sniffle': 82201,\n",
              "  'slimeball': 40957,\n",
              "  'jia': 40958,\n",
              "  'milked': 16828,\n",
              "  'banjoes': 40959,\n",
              "  'jim': 1237,\n",
              "  'workforces': 52348,\n",
              "  'jip': 52349,\n",
              "  'rotweiller': 52350,\n",
              "  'mundaneness': 34763,\n",
              "  \"'ninja'\": 52351,\n",
              "  \"dead'\": 11040,\n",
              "  \"cipriani's\": 40960,\n",
              "  'modestly': 20608,\n",
              "  \"professor'\": 52352,\n",
              "  'shacked': 40961,\n",
              "  'bashful': 34764,\n",
              "  'sorter': 23388,\n",
              "  'overpowering': 16120,\n",
              "  'workmanlike': 18521,\n",
              "  'henpecked': 27662,\n",
              "  'sorted': 18522,\n",
              "  \"jōb's\": 52354,\n",
              "  \"'always\": 52355,\n",
              "  \"'baptists\": 34765,\n",
              "  'dreamcatchers': 52356,\n",
              "  \"'silence'\": 52357,\n",
              "  'hickory': 21929,\n",
              "  'fun\\x97yet': 52358,\n",
              "  'breakumentary': 52359,\n",
              "  'didn': 15496,\n",
              "  'didi': 52360,\n",
              "  'pealing': 52361,\n",
              "  'dispite': 40962,\n",
              "  \"italy's\": 25262,\n",
              "  'instability': 21930,\n",
              "  'quarter': 6539,\n",
              "  'quartet': 12608,\n",
              "  'padmé': 52362,\n",
              "  \"'bleedmedry\": 52363,\n",
              "  'pahalniuk': 52364,\n",
              "  'honduras': 52365,\n",
              "  'bursting': 10786,\n",
              "  \"pablo's\": 41465,\n",
              "  'irremediably': 52367,\n",
              "  'presages': 40963,\n",
              "  'bowlegged': 57832,\n",
              "  'dalip': 65183,\n",
              "  'entering': 6260,\n",
              "  'newsradio': 76172,\n",
              "  'presaged': 54150,\n",
              "  \"giallo's\": 27663,\n",
              "  'bouyant': 40964,\n",
              "  'amerterish': 52368,\n",
              "  'rajni': 18523,\n",
              "  'leeves': 30610,\n",
              "  'macauley': 34767,\n",
              "  'seriously': 612,\n",
              "  'sugercoma': 52369,\n",
              "  'grimstead': 52370,\n",
              "  \"'fairy'\": 52371,\n",
              "  'zenda': 30611,\n",
              "  \"'twins'\": 52372,\n",
              "  'realisation': 17640,\n",
              "  'highsmith': 27664,\n",
              "  'raunchy': 7817,\n",
              "  'incentives': 40965,\n",
              "  'flatson': 52374,\n",
              "  'snooker': 35097,\n",
              "  'crazies': 16829,\n",
              "  'crazier': 14902,\n",
              "  'grandma': 7094,\n",
              "  'napunsaktha': 52375,\n",
              "  'workmanship': 30612,\n",
              "  'reisner': 52376,\n",
              "  \"sanford's\": 61306,\n",
              "  '\\x91doña': 52377,\n",
              "  'modest': 6108,\n",
              "  \"everything's\": 19153,\n",
              "  'hamer': 40966,\n",
              "  \"couldn't'\": 52379,\n",
              "  'quibble': 13001,\n",
              "  'socking': 52380,\n",
              "  'tingler': 21931,\n",
              "  'gutman': 52381,\n",
              "  'lachlan': 40967,\n",
              "  'tableaus': 52382,\n",
              "  'headbanger': 52383,\n",
              "  'spoken': 2847,\n",
              "  'cerebrally': 34768,\n",
              "  \"'road\": 23490,\n",
              "  'tableaux': 21932,\n",
              "  \"proust's\": 40968,\n",
              "  'periodical': 40969,\n",
              "  \"shoveller's\": 52385,\n",
              "  'tamara': 25263,\n",
              "  'affords': 17641,\n",
              "  'concert': 3249,\n",
              "  \"yara's\": 87955,\n",
              "  'someome': 52386,\n",
              "  'lingering': 8424,\n",
              "  \"abraham's\": 41511,\n",
              "  'beesley': 34769,\n",
              "  'cherbourg': 34770,\n",
              "  'kagan': 28624,\n",
              "  'snatch': 9097,\n",
              "  \"miyazaki's\": 9260,\n",
              "  'absorbs': 25264,\n",
              "  \"koltai's\": 40970,\n",
              "  'tingled': 64027,\n",
              "  'crossroads': 19511,\n",
              "  'rehab': 16121,\n",
              "  'falworth': 52389,\n",
              "  'sequals': 52390,\n",
              "  ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRkWp9Y7k696",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYEMYRlloCCN",
        "colab_type": "code",
        "outputId": "edffeb43-e82d-422b-8157-9e5acc8cd7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yokz5i3lYMo",
        "colab_type": "code",
        "outputId": "c6b0cdbb-c626-43ea-ab74-e1f6643d030b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# while we're at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk21Eezboy_s",
        "colab_type": "code",
        "outputId": "6c3d6728-291e-4b38-dfef-7af0c6790256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#making a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9344935]\n",
            "[0.27735958]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etBm7KmoqGHx",
        "colab_type": "code",
        "outputId": "25b2fea1-354f-498d-8898-21f629c5284c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sAUKO-gtvPP",
        "colab_type": "code",
        "outputId": "baab8780-b11f-4f11-e6ff-8c11c83accf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPMXZ04tyBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to upload our own file\n",
        "\"\"\"from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcVdsl4yt2ze",
        "colab_type": "code",
        "outputId": "b0567c0a-9e22-477c-c217-9b93a6de781a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPpq7Iut6cb",
        "colab_type": "code",
        "outputId": "1813bfb3-baa4-4932-def4-7747af48989e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_-QH5eDt8Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdd9SeTit8hi",
        "colab_type": "code",
        "outputId": "15bf0165-4cd6-429d-e6b2-93dc57bb6346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6woSbckOuAzd",
        "colab_type": "code",
        "outputId": "7405122a-5b83-4e44-9dbb-b3d71dba4e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmQYfZLDuBob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnrkJrpj5IOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv76uysI5Ksa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvl_HFd25NPK",
        "colab_type": "code",
        "outputId": "ff43fecc-59bc-486d-9ebb-d38a6f53b286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUJoH33w5Qm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPhywgfU5tXj",
        "colab_type": "code",
        "outputId": "369e9aa1-ffe8-4f1c-835f-d4044dc6f0a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdly0dO85ytX",
        "colab_type": "code",
        "outputId": "b8c28126-fff3-4aff-ebb5-b2055643afb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8FUXI4H53cX",
        "colab_type": "code",
        "outputId": "ccbc424f-ad5d-4a60-96b9-23712bce3065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 4.5842603e-03  1.3953068e-03  1.0012606e-03 ... -2.6670808e-03\n",
            "    4.8370371e-03 -6.0607013e-03]\n",
            "  [ 8.3607286e-03  2.3874948e-03  2.4988861e-03 ... -4.8990510e-03\n",
            "    8.1510842e-03 -1.0426581e-02]\n",
            "  [ 3.9634802e-03  3.7012859e-03  1.2129892e-03 ... -6.3499552e-04\n",
            "    9.6160383e-04 -4.5581739e-03]\n",
            "  ...\n",
            "  [ 1.2419546e-02 -1.3349028e-03 -4.1525415e-03 ... -2.2950685e-03\n",
            "   -5.9643164e-03  5.1625310e-03]\n",
            "  [ 1.2785386e-02  3.9405227e-03 -2.6655709e-03 ... -1.2408644e-03\n",
            "   -5.7659307e-03  2.6604915e-03]\n",
            "  [ 1.0399178e-02  2.2081407e-03 -8.5334033e-03 ...  3.3421633e-03\n",
            "   -5.1507792e-03  1.4777230e-03]]\n",
            "\n",
            " [[-5.0435262e-04 -2.1652088e-03  6.6280511e-04 ... -2.8579048e-04\n",
            "   -3.9311056e-04  6.3278638e-03]\n",
            "  [ 7.0744473e-04 -1.1088157e-02  2.4139243e-03 ... -7.8747142e-03\n",
            "   -6.4899074e-04  4.3420838e-03]\n",
            "  [ 4.8958743e-04 -6.6892132e-03 -1.0411381e-03 ... -8.1809834e-03\n",
            "    5.6384364e-03  6.9516767e-03]\n",
            "  ...\n",
            "  [ 1.8430450e-03 -4.7759432e-04 -2.7263705e-03 ... -4.1753273e-03\n",
            "    4.1605113e-04  9.2719551e-03]\n",
            "  [ 5.2568805e-03 -1.6219721e-03  3.1332327e-03 ... -1.1027986e-03\n",
            "    3.1651654e-03  1.0719438e-02]\n",
            "  [ 6.1518447e-03 -1.0195883e-02  4.5613442e-03 ... -6.9626542e-03\n",
            "    2.3744018e-03  7.5579481e-03]]\n",
            "\n",
            " [[-7.2427618e-04  1.6898336e-03 -3.6094557e-03 ... -2.3501897e-03\n",
            "    6.1765974e-03  3.6388289e-03]\n",
            "  [ 2.7548710e-03 -9.1116136e-04  2.1539950e-03 ... -1.9961037e-04\n",
            "    8.1495885e-03  6.1401855e-03]\n",
            "  [ 5.3499453e-03  3.3490737e-03  6.4391606e-03 ... -4.4634482e-03\n",
            "    2.7378830e-03  1.0118754e-02]\n",
            "  ...\n",
            "  [-1.2941903e-03  1.6009793e-02  6.3975528e-04 ...  7.5733457e-03\n",
            "   -9.5076635e-03  3.0114828e-03]\n",
            "  [ 4.2359084e-03  1.2375897e-02 -2.3023430e-03 ...  5.4042647e-04\n",
            "   -1.1688554e-02 -1.5615635e-03]\n",
            "  [ 1.5644287e-03  7.2015007e-03 -1.7203786e-03 ...  2.1849703e-03\n",
            "   -9.3138888e-03  5.6243399e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.1692558e-05 -1.1587309e-02 -3.2323711e-03 ... -4.3242157e-04\n",
            "    1.4110699e-03 -1.7603982e-03]\n",
            "  [-9.0715662e-04 -9.9623669e-03 -1.7745135e-03 ... -9.3359523e-04\n",
            "    1.3823248e-04  4.9282266e-03]\n",
            "  [-1.0804029e-04 -6.9860430e-03 -2.6904047e-03 ... -1.2872920e-03\n",
            "    4.0790876e-03  4.4540381e-03]\n",
            "  ...\n",
            "  [ 6.3419249e-03 -6.1784512e-03 -2.1290672e-03 ... -4.2605340e-03\n",
            "   -5.1999567e-03  3.5428777e-03]\n",
            "  [ 4.5234491e-03 -6.2066289e-03 -7.3832273e-04 ... -2.9382014e-03\n",
            "   -3.7720464e-03  9.2477128e-03]\n",
            "  [ 8.7248012e-03 -6.5178820e-03  1.4302903e-03 ... -2.1481304e-03\n",
            "   -1.9370541e-03  5.6707938e-03]]\n",
            "\n",
            " [[-5.5470844e-03  3.9491905e-03 -7.5055053e-04 ...  1.3710086e-03\n",
            "   -1.7674081e-03  1.7594205e-03]\n",
            "  [-8.4461644e-03 -2.6576633e-03 -5.4842508e-03 ...  4.1431361e-03\n",
            "   -4.1408851e-03  1.8274784e-04]\n",
            "  [-4.9762237e-03 -1.1296149e-02 -2.7961347e-03 ... -5.4611494e-03\n",
            "   -2.2057232e-03 -2.2395886e-04]\n",
            "  ...\n",
            "  [-1.7594243e-03 -3.2585980e-03  4.4954773e-03 ...  7.0502819e-03\n",
            "   -6.9796788e-03  1.3523976e-02]\n",
            "  [-6.1296253e-03  1.4965516e-04  1.1907944e-02 ...  6.8812808e-03\n",
            "   -4.1768234e-03  1.2135782e-02]\n",
            "  [-1.9844007e-03  5.8090761e-03  9.5308367e-03 ...  6.6680224e-03\n",
            "   -2.5578658e-03  9.7489394e-03]]\n",
            "\n",
            " [[-5.0435262e-04 -2.1652088e-03  6.6280511e-04 ... -2.8579048e-04\n",
            "   -3.9311056e-04  6.3278638e-03]\n",
            "  [-4.7074100e-03  3.7283689e-04  8.1778206e-03 ...  1.4016886e-03\n",
            "    4.6738260e-04  5.5061486e-03]\n",
            "  [-3.8782191e-03  2.4076984e-03  2.8986665e-03 ... -7.2784536e-04\n",
            "    6.8248920e-03  8.5869338e-03]\n",
            "  ...\n",
            "  [ 2.7807907e-03 -4.7584772e-03  5.8109127e-04 ... -5.2578165e-03\n",
            "    2.7010876e-03  1.0506164e-02]\n",
            "  [ 5.6359619e-03 -5.4545687e-03  5.6283069e-03 ... -1.6650716e-03\n",
            "    5.5382261e-03  1.2087533e-02]\n",
            "  [ 6.2316102e-03 -1.3547093e-02  6.4342152e-03 ... -7.2600739e-03\n",
            "    4.6627200e-03  8.8986838e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAqGVO2k55-U",
        "colab_type": "code",
        "outputId": "67e1f69a-ec0e-4617-fa15-39ebd5197d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00458426  0.00139531  0.00100126 ... -0.00266708  0.00483704\n",
            "  -0.0060607 ]\n",
            " [ 0.00836073  0.00238749  0.00249889 ... -0.00489905  0.00815108\n",
            "  -0.01042658]\n",
            " [ 0.00396348  0.00370129  0.00121299 ... -0.000635    0.0009616\n",
            "  -0.00455817]\n",
            " ...\n",
            " [ 0.01241955 -0.0013349  -0.00415254 ... -0.00229507 -0.00596432\n",
            "   0.00516253]\n",
            " [ 0.01278539  0.00394052 -0.00266557 ... -0.00124086 -0.00576593\n",
            "   0.00266049]\n",
            " [ 0.01039918  0.00220814 -0.0085334  ...  0.00334216 -0.00515078\n",
            "   0.00147772]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwNiDTOx58fy",
        "colab_type": "code",
        "outputId": "4e4041b0-7e2f-4b53-98af-3d17f51cec22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 4.5842603e-03  1.3953068e-03  1.0012606e-03 -1.0883755e-03\n",
            " -1.6069482e-04  1.4236621e-03  5.4281433e-03  3.9126808e-03\n",
            "  1.6532218e-03  3.4370690e-03  3.1936629e-04 -3.3490807e-03\n",
            " -7.1061042e-04 -2.1285343e-03  1.5749354e-03 -4.4176560e-03\n",
            " -6.8169292e-03 -1.4768081e-03 -2.9479014e-03  7.8048470e-04\n",
            " -1.6846774e-03 -1.4856020e-03  4.3962407e-03 -4.1605141e-03\n",
            " -5.5116685e-03  5.3180265e-05  2.5392587e-03  3.5015070e-03\n",
            "  1.0304153e-04 -2.6488607e-04  1.0280669e-03  3.3745458e-03\n",
            " -5.3748023e-05  5.3398749e-03  1.0469414e-03 -1.1741404e-03\n",
            "  1.1837165e-03  8.5717533e-05  1.8214770e-03 -3.1604818e-03\n",
            "  2.1184504e-04  3.7598985e-03 -9.7343754e-03 -9.9674240e-04\n",
            "  4.1490071e-03  2.8484599e-03 -4.3091923e-03 -8.4307082e-03\n",
            " -2.7076167e-04 -9.5313013e-04 -3.6597385e-03  1.5404974e-03\n",
            "  1.8288286e-03 -3.1366819e-03 -2.1116617e-03 -2.0738395e-03\n",
            "  3.2301163e-03  1.0542050e-03 -3.8783019e-03 -9.6937083e-04\n",
            " -4.2649033e-03 -1.2432381e-03 -2.6670808e-03  4.8370371e-03\n",
            " -6.0607013e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_cU_zYc5_df",
        "colab_type": "code",
        "outputId": "8d33b2b7-782d-4a96-d691-ba65e189b959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"HtGxABtTI&xL&KJfBqesotFtdM3$ Pn b reIYIsRyJZSMvbmd.a:h?e!HeuUAlG;feEDqRJ?m:VDFBU,LAay;,ZxzG'gykelMlW\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYZJS8q56CqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8_V4Fip6GEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-7dxw7K6Iz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezVFqcbO6LGZ",
        "colab_type": "code",
        "outputId": "8682d3d7-7074-480a-e1ff-50797e7430de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 2.6277\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9191\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.6689\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.5343\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.4521\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.3950\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.3518\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.3134\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2801\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2495\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2188\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1878\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1562\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1231\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0909\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.0545\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.0183\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.9823\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9443\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.9072\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.8712\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.8350\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.8001\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.7683\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.7386\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7099\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.6845\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.6602\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.6386\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.6171\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5992\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5817\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5664\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5538\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5385\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5276\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5165\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.5051\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4979\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4910\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4836\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4763\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4693\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4639\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4570\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4534\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4489\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4440\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4419\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnKK7L2w6QM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFB5Tmdk6S_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txxfrCg_6V2f",
        "colab_type": "code",
        "outputId": "4d6993f4-4f16-469a-cc20-47ea3a760c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-97614ead033d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   def compile(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1225\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m       \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_is_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1616\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m   1617\u001b[0m           filepath.endswith('.hdf5'))\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python._pywrap_checkpoint_reader.Checkp' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyWdr6fv6YUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJQqmCIF6bfp",
        "colab_type": "code",
        "outputId": "770b690a-6d7e-4f94-cef1-25069f001667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: romeo\n",
            "romeof this woman?\n",
            "\n",
            "Nurse:\n",
            "I know him well: you have a stomachlife, when I\n",
            "\n",
            "PARIS:\n",
            "Why, how now, son? what's this?\n",
            "\n",
            "LADY CAPULET:\n",
            "What think you, if you call?\n",
            "\n",
            "KING RICHARD II:\n",
            "Fly to the duke.\n",
            "\n",
            "LUCIO:\n",
            "How now? more behind in golden! woe me not all decore.\n",
            "\n",
            "ROMEO:\n",
            "What, house-bed laid me not? why, 'tis news I have done this the princh and not true.\n",
            "\n",
            "TRANIO:\n",
            "What says he?\n",
            "\n",
            "CATESBY:\n",
            "Here, sir; here had she spenst me where the affection give than guest\n",
            "And by the man that profish'd thus applauant;\n",
            "For thou dost know our vain tyrant.\n",
            "\n",
            "PAULINA:\n",
            "Tut, tult KINGHAM:\n",
            "Think you, my lord, for I no matter,\n",
            "Now much off that title it did sweetly, measure his harvest stripes home: but with a mighty soul,\n",
            "age! I am my neither violent fire,\n",
            "For thy consent to my dife-pirchangemast, seat!\n",
            "The senseless virtue o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPhceGfIBMvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}